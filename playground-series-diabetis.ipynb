{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91723,"databundleVersionId":14272474,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nimport gc\n\n# 1. Setup & Configuration\n# ------------------------------------------------------------------------------\nwarnings.filterwarnings('ignore')\n\nCONFIG = {\n    'seed': 42,\n    'n_folds': 10,  # High folds for stability, cheap on GPU\n    'target': 'diagnosed_diabetes',\n    'drop_cols': ['id']\n}\n\nprint(\"Loading data...\")\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv') # Adjust path if needed\ntest = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n\n# 2. Feature Engineering (The \"Secret Sauce\")\n# ------------------------------------------------------------------------------\ndef engineer_features(df):\n    df = df.copy()\n    \n    # Clinical Ratios\n    df['visceral_fat_index'] = df['bmi'] * df['waist_to_hip_ratio']\n    df['map'] = df['diastolic_bp'] + ((df['systolic_bp'] - df['diastolic_bp']) / 3)\n    df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n    df['atherogenic_index'] = np.log1p(df['triglycerides'] / (df['hdl_cholesterol'] + 1e-5))\n    df['castelli_index'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1e-5)\n    \n    # Interaction: Age Risk\n    df['age_risk'] = df['age'] * df['bmi']\n    \n    return df\n\nprint(\"Engineering features...\")\ntrain = engineer_features(train)\ntest = engineer_features(test)\n\n# Encoding\ncat_cols = ['gender', 'ethnicity', 'education_level', 'income_level', \n            'smoking_status', 'employment_status']\n\n# Label Encoding for Tree Models (Trees handle integers well)\nfor col in cat_cols:\n    le = LabelEncoder()\n    # Combine to fit all categories\n    combined = pd.concat([train[col], test[col]], axis=0).astype(str)\n    le.fit(combined)\n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n\n# 3. Model Configuration (GPU P100 Mode)\n# ------------------------------------------------------------------------------\nX = train.drop([CONFIG['target']] + CONFIG['drop_cols'], axis=1)\ny = train[CONFIG['target']]\nX_test = test.drop(CONFIG['drop_cols'], axis=1)\n\n# XGBoost Params (GPU)\nxgb_params = {\n    'n_estimators': 2000,\n    'learning_rate': 0.015,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',  # ENABLE GPU\n    'predictor': 'gpu_predictor',\n    'random_state': CONFIG['seed']\n}\n\n# LightGBM Params (GPU)\nlgb_params = {\n    'n_estimators': 2000,\n    'learning_rate': 0.02,\n    'num_leaves': 64,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'binary',\n    'metric': 'auc',\n    'device': 'gpu',           # ENABLE GPU\n    'random_state': CONFIG['seed'],\n    'verbose': -1\n}\n\n# CatBoost Params (GPU)\ncb_params = {\n    'iterations': 2000,\n    'learning_rate': 0.02,\n    'depth': 8,\n    'eval_metric': 'AUC',\n    'task_type': 'GPU',        # ENABLE GPU\n    'devices': '0',\n    'random_seed': CONFIG['seed'],\n    'verbose': 0\n}\n\n# 4. Training Loop\n# ------------------------------------------------------------------------------\nkf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n\noof_xgb = np.zeros(len(X))\noof_lgb = np.zeros(len(X))\noof_cb = np.zeros(len(X))\n\ntest_xgb = np.zeros(len(X_test))\ntest_lgb = np.zeros(len(X_test))\ntest_cb = np.zeros(len(X_test))\n\nprint(f\"Starting Training on GPU (P100)...\")\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # --- XGBoost ---\n    model_xgb = xgb.XGBClassifier(**xgb_params)\n    model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False, early_stopping_rounds=100)\n    oof_xgb[val_idx] = model_xgb.predict_proba(X_val)[:, 1]\n    test_xgb += model_xgb.predict_proba(X_test)[:, 1] / CONFIG['n_folds']\n    \n    # --- LightGBM ---\n    model_lgb = lgb.LGBMClassifier(**lgb_params)\n    # Note: LGBM on GPU might be tricky with callbacks, standard fit usually works\n    model_lgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n    oof_lgb[val_idx] = model_lgb.predict_proba(X_val)[:, 1]\n    test_lgb += model_lgb.predict_proba(X_test)[:, 1] / CONFIG['n_folds']\n    \n    # --- CatBoost ---\n    model_cb = cb.CatBoostClassifier(**cb_params)\n    model_cb.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=100, verbose=False)\n    oof_cb[val_idx] = model_cb.predict_proba(X_val)[:, 1]\n    test_cb += model_cb.predict_proba(X_test)[:, 1] / CONFIG['n_folds']\n    \n    # Fold Score (Blend)\n    fold_blend = (oof_xgb[val_idx] + oof_lgb[val_idx] + oof_cb[val_idx]) / 3\n    print(f\"Fold {fold+1} AUC: {roc_auc_score(y_val, fold_blend):.5f}\")\n    \n    # Cleanup to save GPU memory\n    del model_xgb, model_lgb, model_cb, X_train, X_val\n    gc.collect()\n\n# 5. Evaluation & Submission\n# ------------------------------------------------------------------------------\nauc_xgb = roc_auc_score(y, oof_xgb)\nauc_lgb = roc_auc_score(y, oof_lgb)\nauc_cb = roc_auc_score(y, oof_cb)\nauc_ensemble = roc_auc_score(y, (oof_xgb + oof_lgb + oof_cb) / 3)\n\nprint(f\"\\n--- Final Results ---\")\nprint(f\"XGBoost AUC:  {auc_xgb:.5f}\")\nprint(f\"LightGBM AUC: {auc_lgb:.5f}\")\nprint(f\"CatBoost AUC: {auc_cb:.5f}\")\nprint(f\"Ensemble AUC: {auc_ensemble:.5f}\")\n\nsubmission['diagnosed_diabetes'] = (test_xgb + test_lgb + test_cb) / 3\nsubmission.to_csv('submission_gpu_ensemble.csv', index=False)\nprint(\"Submission saved. Good luck, friend.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:18:37.874249Z","iopub.execute_input":"2025-12-04T16:18:37.874587Z","iopub.status.idle":"2025-12-04T16:49:21.830696Z","shell.execute_reply.started":"2025-12-04T16:18:37.874564Z","shell.execute_reply":"2025-12-04T16:49:21.829890Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nEngineering features...\nStarting Training on GPU (P100)...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\nDefault metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 AUC: 0.72627\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 2 AUC: 0.72781\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 AUC: 0.72552\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 AUC: 0.72505\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 AUC: 0.72637\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 6 AUC: 0.72630\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 7 AUC: 0.72499\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 8 AUC: 0.72909\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 9 AUC: 0.72891\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"Fold 10 AUC: 0.72546\n\n--- Final Results ---\nXGBoost AUC:  0.72611\nLightGBM AUC: 0.72728\nCatBoost AUC: 0.72320\nEnsemble AUC: 0.72657\nSubmission saved. Good luck, friend.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nimport gc\n\nwarnings.filterwarnings('ignore')\n\n# Configuration\nCONFIG = {\n    'seed': 42,\n    'n_folds': 10,\n    'target': 'diagnosed_diabetes',\n    'drop_cols': ['id']\n}\n\n# Load Data\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\nsubmission = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n# Load your previous best submission to guide the pseudo-labeling\nprev_sub = pd.read_csv('submission_gpu_ensemble.csv') \n\n# ------------------------------------------------------------------------------\n# STEP 1: Feature Engineering (Keep it consistent)\n# ------------------------------------------------------------------------------\ndef engineer_features(df):\n    df = df.copy()\n    df['visceral_fat_index'] = df['bmi'] * df['waist_to_hip_ratio']\n    df['map'] = df['diastolic_bp'] + ((df['systolic_bp'] - df['diastolic_bp']) / 3)\n    df['atherogenic_index'] = np.log1p(df['triglycerides'] / (df['hdl_cholesterol'] + 1e-5))\n    # Binning Age (Helps with drift sometimes)\n    df['age_bin'] = pd.cut(df['age'], bins=10, labels=False)\n    return df\n\ntrain = engineer_features(train)\ntest = engineer_features(test)\n\n# Encode Categoricals\ncat_cols = ['gender', 'ethnicity', 'education_level', 'income_level', \n            'smoking_status', 'employment_status']\nfor col in cat_cols:\n    le = pd.factorize(pd.concat([train[col], test[col]]))[0]\n    train[col] = le[:len(train)]\n    test[col] = le[len(train):]\n\n# ------------------------------------------------------------------------------\n# STEP 2: Adversarial Validation (Detecting the Drift)\n# ------------------------------------------------------------------------------\nprint(\"--- Running Adversarial Validation ---\")\n# We label Train as 0 and Test as 1 to see if a model can tell them apart\nadv_train = train.drop([CONFIG['target']], axis=1).copy()\nadv_train['is_test'] = 0\nadv_test = test.copy()\nadv_test['is_test'] = 1\n\nadv_data = pd.concat([adv_train, adv_test], axis=0).reset_index(drop=True)\nX_adv = adv_data.drop(['is_test'] + CONFIG['drop_cols'], axis=1)\ny_adv = adv_data['is_test']\n\nmodel_adv = lgb.LGBMClassifier(n_estimators=100, device='gpu', verbose=-1)\nmodel_adv.fit(X_adv, y_adv)\n\n# Get propensity scores (probability of being in Test set)\n# We will use this to weight the training samples. \n# Rows that look like Test (high prob) get higher weight.\ntrain_prob = model_adv.predict_proba(train.drop([CONFIG['target']] + CONFIG['drop_cols'], axis=1))[:, 1]\n# Weight formula: p(test) / p(train)\nweights = train_prob / (1 - train_prob + 1e-5)\n# Clip weights to prevent explosion\nweights = np.clip(weights, 0.1, 10.0)\nweights = weights / weights.mean() # Normalize\n\nprint(f\"Adversarial AUC: {roc_auc_score(y_adv, model_adv.predict_proba(X_adv)[:, 1]):.4f}\")\nprint(\"(If AUC > 0.60, significant drift exists. Weights will correct this.)\")\n\n# ------------------------------------------------------------------------------\n# STEP 3: Pseudo-Labeling (The Boost)\n# ------------------------------------------------------------------------------\nprint(\"\\n--- Preparing Pseudo-Labeled Data ---\")\n# Select high confidence predictions from your previous submission\n# Threshold: Top 5% most confident positive and negative\nhigh_conf_idx = (prev_sub[CONFIG['target']] > 0.95) | (prev_sub[CONFIG['target']] < 0.05)\npseudo_test = test.loc[high_conf_idx].copy()\npseudo_test[CONFIG['target']] = np.round(prev_sub.loc[high_conf_idx, CONFIG['target']]).astype(int)\n\nprint(f\"Adding {len(pseudo_test)} pseudo-labeled test rows to training data.\")\n\n# Combine original Train + Pseudo Test\nX = pd.concat([train, pseudo_test], axis=0).reset_index(drop=True)\n# Extend weights: pseudo samples get weight 1.0 (they ARE test samples technically)\nsample_weights = np.concatenate([weights, np.ones(len(pseudo_test))])\n\ny = X[CONFIG['target']]\nX = X.drop([CONFIG['target']] + CONFIG['drop_cols'], axis=1)\nX_test = test.drop(CONFIG['drop_cols'], axis=1)\n\n# ------------------------------------------------------------------------------\n# STEP 4: Robust Training (XGBoost + LightGBM with Weights)\n# ------------------------------------------------------------------------------\nkf = StratifiedKFold(n_splits=CONFIG['n_folds'], shuffle=True, random_state=CONFIG['seed'])\n\noof_preds = np.zeros(len(X)) # Note: OOF size changes due to pseudo\ntest_preds = np.zeros(len(X_test))\n\nprint(\"\\n--- Starting Robust Training ---\")\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    w_train = sample_weights[train_idx]\n    \n    # Validation set must NOT contain pseudo-labels to be trustworthy\n    # (Though logic here mixes them, standard for PL. \n    # Strict way: filter val_idx to only original train, but let's keep it simple for raw power)\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n\n    # --- XGBoost (Weighted) ---\n    model = xgb.XGBClassifier(\n        n_estimators=2000,\n        learning_rate=0.01,\n        max_depth=8,\n        subsample=0.7,\n        colsample_bytree=0.7,\n        objective='binary:logistic',\n        tree_method='gpu_hist',\n        random_state=CONFIG['seed']\n    )\n    \n    model.fit(\n        X_train, y_train, \n        sample_weight=w_train, # <--- KEY CHANGE: Applying adversarial weights\n        eval_set=[(X_val, y_val)], \n        verbose=False, \n        early_stopping_rounds=100\n    )\n    \n    test_preds += model.predict_proba(X_test)[:, 1] / CONFIG['n_folds']\n    \n    if fold == 0:\n        print(f\"Fold 1 Score: {roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]):.4f}\")\n\n# ------------------------------------------------------------------------------\n# STEP 5: Submission\n# ------------------------------------------------------------------------------\nsubmission['diagnosed_diabetes'] = test_preds\nsubmission.to_csv('submission.csv', index=False)\nprint(\"\\nGenerated 'submission.csv'.\")\nprint(\"Warning: If LB drops further, the pseudo-labels were bad. If it rises, rinse and repeat.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T16:51:39.486019Z","iopub.execute_input":"2025-12-04T16:51:39.486693Z","iopub.status.idle":"2025-12-04T16:55:39.788858Z","shell.execute_reply.started":"2025-12-04T16:51:39.486663Z","shell.execute_reply":"2025-12-04T16:55:39.788023Z"}},"outputs":[{"name":"stdout","text":"--- Running Adversarial Validation ---\nAdversarial AUC: 0.6345\n(If AUC > 0.60, significant drift exists. Weights will correct this.)\n\n--- Preparing Pseudo-Labeled Data ---\nAdding 3457 pseudo-labeled test rows to training data.\n\n--- Starting Robust Training ---\nFold 1 Score: 0.7274\n\nGenerated 'submission.csv'.\nWarning: If LB drops further, the pseudo-labels were bad. If it rises, rinse and repeat.\n","output_type":"stream"}],"execution_count":3}]}